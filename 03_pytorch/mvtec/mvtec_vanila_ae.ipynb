{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1902bff4",
   "metadata": {},
   "source": [
    "## MVTec Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51babca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_msssim import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5f86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "class MVTec(Dataset):\n",
    "    def __init__(self, data_dir, categories, split, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for category in categories:\n",
    "            category_path = os.path.join(data_dir, category, split)\n",
    "            if split == \"train\":\n",
    "                label = 0\n",
    "                for path in glob(os.path.join(category_path, \"good\", \"*.png\")):\n",
    "                    self.image_paths.append(path)\n",
    "                    self.labels.append(label)\n",
    "            else:\n",
    "                for subfolder in os.listdir(category_path):\n",
    "                    label = 0 if subfolder == \"good\" else 1\n",
    "                    for path in glob(os.path.join(category_path, subfolder, \"*.png\")):\n",
    "                        self.image_paths.append(path)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        # return {\"image\": image, \"label\": label, \"path\": path}\n",
    "        return {\"image\": image, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6efbb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        super().__init__()\n",
    "        self.deconv_block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.deconv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b842c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanilaEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 512),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.conv_blocks(x)\n",
    "        pooled = self.pool(features)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        latent = self.fc(pooled)\n",
    "        return latent, features\n",
    "\n",
    "\n",
    "class VanilaDecoder(nn.Module):\n",
    "    def __init__(self, out_channels=3, latent_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, 512*8*8)\n",
    "        self.unflatten = nn.Unflatten(1, (512, 8, 8))\n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            DeconvBlock(512, 256),\n",
    "            DeconvBlock(256, 128),\n",
    "            DeconvBlock(128, 64),\n",
    "            DeconvBlock(64, 32),\n",
    "            nn.ConvTranspose2d(32, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, latent):\n",
    "        x = self.fc(latent)                     # (B, 512 * 8 * 8)\n",
    "        x = self.unflatten(x)                   # (B, 512, 8, 8)\n",
    "        reconstructed = self.deconv_blocks(x)\n",
    "        return reconstructed\n",
    "\n",
    "class VanilaAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent, features = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer\n",
    "def train(model, data_loader, loss_fn, optimizer, metrics={}):\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "\n",
    "    functions = {\"loss\": loss_fn}\n",
    "    functions.update(metrics)\n",
    "    results = {name: 0.0 for name in functions.keys()}\n",
    "\n",
    "    with tqdm(data_loader, desc=\"Training\", leave=False, file=sys.stdout,\n",
    "              dynamic_ncols=True, ncols=100, ascii=True) as pbar:\n",
    "        for cnt, data in enumerate(pbar):\n",
    "            images = data['image'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "\n",
    "            normal_mask = labels == 0\n",
    "            if not normal_mask.any():\n",
    "                continue\n",
    "\n",
    "            normal_images = images[normal_mask]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, latent, features = model(normal_images)\n",
    "            loss = loss_fn(pred, normal_images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            results[\"loss\"] += loss.item()\n",
    "            for name, func in functions.items():\n",
    "                if name != \"loss\":\n",
    "                    results[name] += func(pred, normal_images).item()\n",
    "\n",
    "            pbar.set_postfix({k: f\"{v/(cnt + 1):.3f}\" for k, v in results.items()})\n",
    "\n",
    "    return {k: v/len(data_loader) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23820b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, loss_fn, metrics={}):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    functions = {\"loss\": loss_fn}\n",
    "    functions.update(metrics)\n",
    "    results = {name: 0.0 for name in functions.keys()}\n",
    "\n",
    "    with tqdm(data_loader, desc=\"Evaluation\", leave=False, file=sys.stdout,\n",
    "            dynamic_ncols=True,\n",
    "            ncols=100, ascii=True) as pbar:\n",
    "        for cnt, data in enumerate(pbar):\n",
    "            images = data['image'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "\n",
    "            normal_mask = labels == 0\n",
    "            if not normal_mask.any():\n",
    "                continue\n",
    "\n",
    "            normal_images = images[normal_mask]\n",
    "            pred, latent, features = model(normal_images)\n",
    "            loss = loss_fn(pred, normal_images)\n",
    "\n",
    "            results[\"loss\"] += loss.item()\n",
    "            for name, func in functions.items():\n",
    "                if name != \"loss\":\n",
    "                    results[name] += func(pred, normal_images).item()\n",
    "\n",
    "            pbar.set_postfix({k: f\"{v/(cnt + 1):.3f}\" for k, v in results.items()})\n",
    "\n",
    "    return {k: v/len(data_loader) for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e79b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(dataset, valid_ratio, seed=42):\n",
    "    data_size = len(dataset)\n",
    "    valid_size = int(data_size * valid_ratio)\n",
    "    train_size = data_size - valid_size\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    train_subset, valid_subset = random_split(dataset, [train_size, valid_size])\n",
    "    return train_subset.indices, valid_subset.indices\n",
    "\n",
    "def recon_loss(pred, target):\n",
    "    bce = nn.BCELoss()\n",
    "    return 0.5 * (1 - ssim(pred, target)) + 0.5 * bce(pred, target)\n",
    "\n",
    "def binary_accuracy(x_pred, x_true):\n",
    "    return torch.eq(x_pred.round(), x_true.round()).float().mean()\n",
    "\n",
    "def psnr(pred, target):\n",
    "    mse = nn.MSELoss()(pred, target)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 10 * torch.log10(1.0 ** 2 / mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b27622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "img_size = 256\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "## Augmentations\n",
    "train_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((img_size, img_size)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((img_size, img_size)),\n",
    "    T.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loaders\n",
    "data_dir = '/mnt/d/datasets/mvtec'\n",
    "categories = ['bottle', 'cable', 'capsule', 'carpet', 'grid',\n",
    "                'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',\n",
    "                'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "# categories = ['bottle', 'grid', 'tile']\n",
    "\n",
    "train_dataset = MVTec(data_dir, categories, split=\"train\", transform=train_transform)\n",
    "valid_dataset = MVTec(data_dir, categories, split=\"train\", transform=test_transform)\n",
    "train_indices, valid_indices = split_train_valid(train_dataset, valid_ratio=0.2)\n",
    "\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "valid_dataset = Subset(valid_dataset, valid_indices)\n",
    "test_dataset  = MVTec(data_dir, categories, split=\"test\", transform=test_transform)\n",
    "\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True, \"drop_last\": True, \"persistent_workers\": True}\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, **kwargs)\n",
    "valid_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb5c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = VanilaEncoder(in_channels=3, latent_dim=512)\n",
    "decoder = VanilaDecoder(out_channels=3, latent_dim=512)\n",
    "model = VanilaAutoEncoder(encoder, decoder).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "metrics = {\"mse\": nn.MSELoss(),\n",
    "        #    \"bce\": nn.BCELoss(),\n",
    "        #    \"acc\": binary_accuracy,\n",
    "           \"ssim\": ssim,\n",
    "           \"psnr\": psnr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b79c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1/10] loss=0.059, mse=0.059, ssim=0.995, psnr=12.457 | val_loss=0.051, val_mse=0.051, val_ssim=0.995, val_psnr=13.005 (19s)\n",
      "[Epoch  2/10] loss=0.039, mse=0.039, ssim=0.997, psnr=14.117 | val_loss=0.036, val_mse=0.036, val_ssim=0.997, val_psnr=14.488 (16s)\n",
      "[Epoch  3/10] loss=0.034, mse=0.034, ssim=0.997, psnr=14.716 | val_loss=0.034, val_mse=0.034, val_ssim=0.997, val_psnr=14.674 (19s)\n",
      "[Epoch  4/10] loss=0.033, mse=0.033, ssim=0.997, psnr=14.837 | val_loss=0.031, val_mse=0.031, val_ssim=0.997, val_psnr=15.068 (16s)\n",
      "[Epoch  5/10] loss=0.030, mse=0.030, ssim=0.997, psnr=15.190 | val_loss=0.028, val_mse=0.028, val_ssim=0.998, val_psnr=15.509 (16s)\n",
      "[Epoch  6/10] loss=0.027, mse=0.027, ssim=0.998, psnr=15.687 | val_loss=0.026, val_mse=0.026, val_ssim=0.998, val_psnr=15.921 (18s)\n",
      "[Epoch  7/10] loss=0.026, mse=0.026, ssim=0.998, psnr=15.812 | val_loss=0.025, val_mse=0.025, val_ssim=0.998, val_psnr=16.011 (16s)\n",
      "[Epoch  8/10] loss=0.025, mse=0.025, ssim=0.998, psnr=16.000 | val_loss=0.025, val_mse=0.025, val_ssim=0.998, val_psnr=15.981 (19s)\n",
      "[Epoch  9/10] loss=0.024, mse=0.024, ssim=0.998, psnr=16.152 | val_loss=0.024, val_mse=0.024, val_ssim=0.998, val_psnr=16.196 (16s)\n",
      "[Epoch 10/10] loss=0.024, mse=0.024, ssim=0.998, psnr=16.169 | val_loss=0.023, val_mse=0.023, val_ssim=0.998, val_psnr=16.395 (19s)\n",
      ">> Test: test_loss=0.006, test_mse=0.006, test_ssim=0.470, test_psnr=8.819                                 \n"
     ]
    }
   ],
   "source": [
    "## Training Loop\n",
    "history = {\"loss\": []}\n",
    "history.update({name: [] for name in metrics.keys()})\n",
    "history.update({f\"val_{name}\": [] for name in history.keys()})\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time()\n",
    "\n",
    "    ## Training\n",
    "    train_results = train(model, train_loader, loss_fn, optimizer, metrics=metrics)\n",
    "    train_desc = ', '.join([f\"{k}={v:.3f}\" for k, v in train_results.items()])\n",
    "\n",
    "    for name, value in train_results.items():\n",
    "        history[name].append(value)\n",
    "\n",
    "    ## Validation\n",
    "    valid_results = evaluate(model, valid_loader, loss_fn, metrics=metrics)\n",
    "    valid_desc = ', '.join([f\"val_{k}={v:.3f}\" for k, v in valid_results.items()])\n",
    "\n",
    "    for name, value in valid_results.items():\n",
    "        history[f\"val_{name}\"].append(value)\n",
    "\n",
    "    epoch_time = time() - start_time\n",
    "    print(f\"[Epoch {epoch:2d}/{num_epochs}] {train_desc} | {valid_desc} ({epoch_time:.0f}s)\")\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "test_results = evaluate(model, test_loader, loss_fn, metrics=metrics)\n",
    "test_desc = ', '.join([f\"test_{k}={v:.3f}\" for k, v in test_results.items()])\n",
    "print(f\">> Test: {test_desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Category: bottle ===\n",
      "                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nampl/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 3, 256, 256])) that is different to the input size (torch.Size([16, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     57\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n\u001b[1;32m     62\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m: cat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics})\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, loss_fn, optimizer, metrics)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m pred, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m model(normal_images)\n\u001b[0;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/nn/functional.py:3884\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3882\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3884\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    y_true, y_score = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            imgs = data['image'].to(device)\n",
    "            labels = data['label'].cpu().numpy()  # 0=good, 1=defect\n",
    "            \n",
    "            recon = model(imgs)\n",
    "            loss = torch.mean((imgs - recon)**2, dim=[1,2,3])  # per-image reconstruction error\n",
    "            \n",
    "            y_true.extend(labels)\n",
    "            y_score.extend(loss.cpu().numpy())\n",
    "    \n",
    "    # AUROC & AUPR\n",
    "    auroc = roc_auc_score(y_true, y_score)\n",
    "    aupr  = average_precision_score(y_true, y_score)\n",
    "    \n",
    "    # threshold: 95th percentile\n",
    "    threshold = np.percentile(y_score, 95)\n",
    "    y_pred = (np.array(y_score) > threshold).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return {\"AUROC\": auroc, \"AUPR\": aupr, \"ACC\": acc, \"F1\": f1}\n",
    "\n",
    "for cat in categories:\n",
    "    print(f\"=== Category: {cat} ===\")\n",
    "    \n",
    "    train_dataset = MVTec(data_dir, categories=[cat], split=\"train\", transform=train_transform)\n",
    "    test_dataset  = MVTec(data_dir, categories=[cat], split=\"test\", transform=test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # model = VanilaAutoEncoder().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train(model, train_loader, criterion, optimizer, metrics=metrics)\n",
    "    \n",
    "    metrics = evaluate(model, test_loader, criterion)\n",
    "    results.append({\"category\": cat, **metrics})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "display(df_results)\n",
    "\n",
    "# Plot AUROC for each category\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(df_results['category'], df_results['AUROC'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"AUROC\")\n",
    "plt.title(\"MVTec Category-wise AUROC with Vanilla AE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
