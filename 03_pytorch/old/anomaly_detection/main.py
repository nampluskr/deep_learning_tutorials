"""
ë©”ì¸ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ ë° ì‹¤í—˜ ê´€ë¦¬
"""

import os
import numpy as np
import torch

# ê° ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ import
from mvtec import get_transforms, get_dataloaders
from vanila_ae import VanillaAutoEncoder  
from train import get_device, train_model, evaluate_model
from utils import (
    visualize_training_history,
    visualize_reconstruction_examples, 
    visualize_score_distribution,
    create_results_summary_table,
    plot_category_comparison
)


def main_pipeline(data_dir, categories=None, num_epochs=50, batch_size=32, 
                 latent_dim=512, img_size=256, normalize=False,
                 save_models=True, save_results=True):
    """
    ë©”ì¸ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸
    
    Args:
        data_dir: MVTec ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        categories: ì‹¤í—˜í•  ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        num_epochs: í›ˆë ¨ ì—í¬í¬ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        latent_dim: ìž ìž¬ ê³µê°„ ì°¨ì›
        img_size: ì´ë¯¸ì§€ í¬ê¸°
        normalize: ImageNet ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        save_models: ëª¨ë¸ ì €ìž¥ ì—¬ë¶€
        save_results: ê²°ê³¼ ì €ìž¥ ì—¬ë¶€
    
    Returns:
        all_category_results: ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        results_df: ê²°ê³¼ DataFrame
    """
    
    # ì„¤ì • ë° ì´ˆê¸°í™”
    device = get_device(seed=42)
    train_transform, test_transform = get_transforms(img_size=img_size, normalize=normalize)
    
    # MVTec ì¹´í…Œê³ ë¦¬ ì •ì˜ (ì „ì²´ ë˜ëŠ” ì„ íƒ)
    if categories is None:
        categories = [
            'bottle', 'cable', 'capsule', 'carpet', 'grid',
            'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 
            'tile', 'toothbrush', 'transistor', 'wood', 'zipper'
        ]
    
    all_category_results = []
    
    print(f"Starting MVTec Anomaly Detection Evaluation")
    print(f"Total categories: {len(categories)}")
    print(f"Device: {device}")
    print(f"Epochs: {num_epochs}, Batch size: {batch_size}")
    print(f"Latent dimension: {latent_dim}, Image size: {img_size}")
    print(f"Normalize: {normalize}")
    print("="*60)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì‹¤í—˜
    for i, category in enumerate(categories):
        print(f"\nðŸ” Processing Category {i+1}/{len(categories)}: {category.upper()}")
        print("-" * 50)
        
        try:
            # 1. ë°ì´í„° ë¡œë” ìƒì„±
            print("Creating dataloaders...")
            train_loader, valid_loader, test_loader = get_dataloaders(
                data_dir, category, batch_size,
                train_transform=train_transform, 
                test_transform=test_transform
            )
            
            # 2. ëª¨ë¸ ì´ˆê¸°í™”
            print("Initializing model...")
            model = VanillaAutoEncoder(latent_dim=latent_dim).to(device)
            total_params = sum(p.numel() for p in model.parameters())
            model_size_mb = total_params * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
            print(f"Model parameters: {total_params:,}")
            print(f"Model size: {model_size_mb:.1f} MB")
            
            # 3. ëª¨ë¸ í›ˆë ¨
            print("Training model...")
            trained_model, history = train_model(
                model, train_loader, valid_loader, device, num_epochs=num_epochs
            )
            
            # 4. í…ŒìŠ¤íŠ¸ í‰ê°€
            print("Evaluating model...")
            results, detailed_results = evaluate_model(trained_model, test_loader, device)
            
            # 5. ê²°ê³¼ ì €ìž¥
            results['Category'] = category
            all_category_results.append(results)
            
            # 6. ì‹œê°í™”
            print("Creating visualizations...")
            try:
                visualize_training_history(history, category)
                visualize_reconstruction_examples(detailed_results, category)
                visualize_score_distribution(detailed_results, category)
            except Exception as viz_error:
                print(f"Warning: Visualization error for {category}: {viz_error}")
            
            # 7. ê²°ê³¼ ì¶œë ¥
            print(f"\nðŸ“Š Results for {category}:")
            print(f"  AUROC: {results['AUROC']:.4f}")
            print(f"  AUPR:  {results['AUPR']:.4f}")
            print(f"  F1:    {results['F1-Score']:.4f}")
            print(f"  Accuracy: {results['Accuracy']:.4f}")
            print(f"  Threshold: {results['Threshold']:.6f}")
            
            # 8. ëª¨ë¸ ì €ìž¥ (ì„ íƒì‚¬í•­)
            if save_models:
                os.makedirs("models", exist_ok=True)
                model_path = f"models/vanilla_ae_{category}.pth"
                torch.save(trained_model.state_dict(), model_path)
                print(f"Model saved: {model_path}")
            
        except Exception as e:
            print(f"âŒ Error processing {category}: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # ì‹¤íŒ¨í•œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë”ë¯¸ ê²°ê³¼ ì¶”ê°€
            dummy_results = {
                'Category': category,
                'AUROC': 0.0,
                'AUPR': 0.0,
                'Accuracy': 0.0,
                'Precision': 0.0,
                'Recall': 0.0,
                'F1-Score': 0.0,
                'Threshold': 0.0,
                'Normal_Score_Mean': 0.0,
                'Normal_Score_Std': 0.0,
                'Anomaly_Score_Mean': 0.0
            }
            all_category_results.append(dummy_results)
            continue
    
    # ì „ì²´ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
    print("\n" + "="*60)
    print("ðŸ“ˆ FINAL RESULTS SUMMARY")
    print("="*60)
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    try:
        results_df = create_results_summary_table(all_category_results)
    except Exception as e:
        print(f"Error creating summary table: {e}")
        import pandas as pd
        results_df = pd.DataFrame(all_category_results)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
    try:
        plot_category_comparison(all_category_results)
    except Exception as e:
        print(f"Error creating comparison plot: {e}")
    
    # ê²°ê³¼ ì €ìž¥
    if save_results:
        try:
            results_df.to_csv("mvtec_vanilla_ae_results.csv", index=False)
            print("Results saved to 'mvtec_vanilla_ae_results.csv'")
        except Exception as e:
            print(f"Error saving results: {e}")
    
    # ìµœê³ /ìµœì € ì„±ëŠ¥ ì¹´í…Œê³ ë¦¬ ë¶„ì„
    if len(all_category_results) > 0:
        # ì‹¤íŒ¨í•˜ì§€ ì•Šì€ ê²°ê³¼ë“¤ë§Œ í•„í„°ë§
        valid_results = [r for r in all_category_results if r['AUROC'] > 0]
        
        if len(valid_results) > 0:
            auroc_scores = [r['AUROC'] for r in valid_results]
            best_auroc_idx = np.argmax(auroc_scores)
            worst_auroc_idx = np.argmin(auroc_scores)
            
            print(f"\nðŸ† Best performing category: {valid_results[best_auroc_idx]['Category']} "
                  f"(AUROC: {valid_results[best_auroc_idx]['AUROC']:.4f})")
            print(f"ðŸ“‰ Challenging category: {valid_results[worst_auroc_idx]['Category']} "
                  f"(AUROC: {valid_results[worst_auroc_idx]['AUROC']:.4f})")
            
            overall_mean_auroc = np.mean(auroc_scores)
            print(f"ðŸ“Š Overall mean AUROC: {overall_mean_auroc:.4f}")
            
            # ì„±ëŠ¥ ë¶„í¬ ë¶„ì„
            excellent_count = sum(1 for score in auroc_scores if score >= 0.9)
            good_count = sum(1 for score in auroc_scores if 0.8 <= score < 0.9)
            fair_count = sum(1 for score in auroc_scores if 0.7 <= score < 0.8)
            poor_count = sum(1 for score in auroc_scores if score < 0.7)
            
            print(f"\nðŸ“ˆ Performance Distribution:")
            print(f"  Excellent (â‰¥0.9): {excellent_count}/{len(valid_results)} categories")
            print(f"  Good (0.8-0.9): {good_count}/{len(valid_results)} categories")
            print(f"  Fair (0.7-0.8): {fair_count}/{len(valid_results)} categories")
            print(f"  Poor (<0.7): {poor_count}/{len(valid_results)} categories")
    
    return all_category_results, results_df


def quick_test(data_dir, test_categories=None, epochs=5):
    """
    ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ê°œë°œ ë° ë””ë²„ê¹…ìš©)
    
    Args:
        data_dir: MVTec ë°ì´í„°ì…‹ ê²½ë¡œ
        test_categories: í…ŒìŠ¤íŠ¸í•  ì¹´í…Œê³ ë¦¬ë“¤ (Noneì´ë©´ ['bottle'])
        epochs: í›ˆë ¨ ì—í¬í¬ ìˆ˜
    """
    if test_categories is None:
        test_categories = ['bottle']
    
    print("ðŸš€ Quick Test Mode")
    print(f"Testing categories: {test_categories}")
    print(f"Epochs: {epochs}")
    
    results, df = main_pipeline(
        data_dir=data_dir,
        categories=test_categories,
        num_epochs=epochs,
        batch_size=16,  # ìž‘ì€ ë°°ì¹˜ í¬ê¸°
        img_size=256,
        normalize=False,  # ì •ê·œí™” ì‚¬ìš© ì•ˆí•¨
        save_models=False,
        save_results=False
    )
    
    print("\nâœ… Quick test completed!")
    return results, df


def test_experiment(data_dir):
    """
    ì¤‘ê°„ í¬ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í—˜ (3ê°œ ì¹´í…Œê³ ë¦¬)
    
    Args:
        data_dir: MVTec ë°ì´í„°ì…‹ ê²½ë¡œ
    """
    print("ðŸ§ª Test Experiment")
    print("Testing with 3 categories...")
    
    test_categories = ['bottle', 'grid', 'tile']
    
    results, df = main_pipeline(
        data_dir=data_dir,
        categories=test_categories,
        num_epochs=15,
        batch_size=24,
        img_size=256,
        normalize=False,
        save_models=True,
        save_results=True
    )
    
    print("\nðŸŽ¯ Test experiment completed!")
    return results, df


def full_experiment(data_dir):
    """
    ì „ì²´ MVTec ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì™„ì „í•œ ì‹¤í—˜
    
    Args:
        data_dir: MVTec ë°ì´í„°ì…‹ ê²½ë¡œ
    """
    print("ðŸ”¬ Full MVTec Experiment")
    print("This will take several hours...")
    
    results, df = main_pipeline(
        data_dir=data_dir,
        categories=None,  # ì „ì²´ ì¹´í…Œê³ ë¦¬
        num_epochs=50,
        batch_size=32,
        latent_dim=512,
        img_size=256,
        normalize=False,
        save_models=True,
        save_results=True
    )
    
    print("\nðŸŽ‰ Full experiment completed!")
    return results, df


def experiment_with_normalization(data_dir):
    """
    ImageNet ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜
    
    Args:
        data_dir: MVTec ë°ì´í„°ì…‹ ê²½ë¡œ
    """
    print("ðŸ” Experiment with ImageNet Normalization")
    
    test_categories = ['bottle', 'grid', 'tile']
    
    results, df = main_pipeline(
        data_dir=data_dir,
        categories=test_categories,
        num_epochs=20,
        batch_size=32,
        img_size=256,
        normalize=True,  # ImageNet ì •ê·œí™” ì‚¬ìš©
        save_models=True,
        save_results=True
    )
    
    print("\nðŸ“Š Normalization experiment completed!")
    return results, df


# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    DATA_DIR = "/path/to/mvtec_anomaly_detection"
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    RUN_MODE = "quick"  # "quick", "test", "full", "norm" ì¤‘ ì„ íƒ
    
    print("="*60)
    print("MVTec Anomaly Detection with Vanilla AutoEncoder")
    print("="*60)
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    try:
        if RUN_MODE == "quick":
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (1ê°œ ì¹´í…Œê³ ë¦¬, 5 ì—í¬í¬)
            results, df = quick_test(DATA_DIR, ['bottle'], epochs=5)
            
        elif RUN_MODE == "test":
            # ì¤‘ê°„ í…ŒìŠ¤íŠ¸ (3ê°œ ì¹´í…Œê³ ë¦¬, 15 ì—í¬í¬)
            results, df = test_experiment(DATA_DIR)
            
        elif RUN_MODE == "full":
            # ì „ì²´ ì‹¤í—˜ (ëª¨ë“  ì¹´í…Œê³ ë¦¬, 50 ì—í¬í¬)
            results, df = full_experiment(DATA_DIR)
            
        elif RUN_MODE == "norm":
            # ì •ê·œí™” ì‹¤í—˜ (3ê°œ ì¹´í…Œê³ ë¦¬, ImageNet ì •ê·œí™” ì‚¬ìš©)
            results, df = experiment_with_normalization(DATA_DIR)
            
        else:
            print(f"Unknown run mode: {RUN_MODE}")
            print("Available modes: 'quick', 'test', 'full', 'norm'")
    
    except Exception as e:
        print(f"\nâŒ Experiment failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*60)
    print("ðŸŽ¯ Experiment completed!")
    print("Check the generated visualizations and results.")
    print("="*60)